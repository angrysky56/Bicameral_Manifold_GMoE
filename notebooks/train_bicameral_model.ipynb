{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bicameral Manifold Training (Colab Edition)\n",
    "\n",
    "This notebook trains the **SmolLM3-3B** model to differentiate into two distinct manifolds:\n",
    "1.  **Logic Manifold:** Fractal Dimension $D_H \\approx 1.5$ (Correlated, Low Entropy)\n",
    "2.  **Creative Manifold:** Fractal Dimension $D_H \\approx 2.5$ (Expansive, High Entropy)\n",
    "\n",
    "It uses the **Neural Fractal Estimator (NFE)** critic you trained previously to enforce these topological constraints.\n",
    "\n",
    "### Pre-requisites\n",
    "1.  You must have `nfe_critic.pt` (trained weights from the previous notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54644057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment & Dependencies\n",
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Dynamic Path Setup\n",
    "current_dir = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "for parent in [current_dir] + list(current_dir.parents):\n",
    "    if (parent / \"core\").exists():\n",
    "        project_root = parent\n",
    "        break\n",
    "\n",
    "if project_root:\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    print(f\"Project root found and added to path: {project_root}\")\n",
    "else:\n",
    "    if not os.path.exists(\"Bicameral_Manifold_GMoE\") and not os.path.exists(\"core\"):\n",
    "        print(\"Cloning repository...\")\n",
    "        get_ipython().system('git clone https://github.com/angrysky56/Bicameral_Manifold_GMoE.git')\n",
    "        get_ipython().run_line_magic('cd', 'Bicameral_Manifold_GMoE')\n",
    "        sys.path.append(\".\")\n",
    "\n",
    "# 2. Install/Fix Dependencies\n",
    "if importlib.util.find_spec(\"tensorflow\") is not None:\n",
    "    print(\"TensorFlow found. Uninstalling to prevent conflicts with Transformers...\")\n",
    "    get_ipython().run_line_magic('pip', 'uninstall -y tensorflow')\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None or importlib.util.find_spec(\"transformers\") is None:\n",
    "    get_ipython().run_line_magic('pip', 'install -q torch transformers accelerate safetensors numpy tqdm matplotlib')\n",
    "\n",
    "# 3. Check Hardware Acceleration (GPU or TPU)\n",
    "import torch\n",
    "device_found = False\n",
    "\n",
    "# Check TPU (PyTorch XLA)\n",
    "if importlib.util.find_spec(\"torch_xla\") is not None:\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        device = xm.xla_device()\n",
    "        print(f\"\\nSUCCESS: TPU Detected: {device}\")\n",
    "        device_found = True\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# Check CUDA\n",
    "if not device_found and torch.cuda.is_available():\n",
    "    print(f\"\\nSUCCESS: GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    device_found = True\n",
    "\n",
    "if not device_found:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARNING: NEITHER CUDA GPU NOR TPU DETECTED!\")\n",
    "    print(\"Training will be extremely slow on CPU.\")\n",
    "    print(\"If on Colab: Runtime -> Change runtime type -> T4 GPU or TPU v2\")\n",
    "    print(\"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e8a66",
   "metadata": {},
   "source": [
    "## 2. Upload Trained NFE Critic\n",
    "Upload your `nfe_critic.pt` file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create checkpoint dir\n",
    "os.makedirs(\"models/checkpoints\", exist_ok=True)\n",
    "\n",
    "# Handle NFE Critic File\n",
    "critic_filename = \"nfe_critic.pt\"\n",
    "target_path = f\"models/checkpoints/{critic_filename}\"\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Google Colab detected. Please upload nfe_critic.pt\")\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        shutil.move(filename, target_path)\n",
    "        print(f\"Saved {filename} to {target_path}\")\n",
    "except ImportError:\n",
    "    print(\"Local environment detected (no google.colab).\")\n",
    "    # Check local paths\n",
    "    possible_paths = [\n",
    "        Path(critic_filename),\n",
    "        Path(\"..\") / critic_filename,\n",
    "        # Check if project_root is defined in previous cell context, otherwise guess\n",
    "        Path(\"../nfe_critic.pt\"), \n",
    "        Path(\"nfe_critic.pt\")\n",
    "    ]\n",
    "    \n",
    "    found = False\n",
    "    for p in possible_paths:\n",
    "        if p and p.exists():\n",
    "            print(f\"Found {critic_filename} at {p}\")\n",
    "            shutil.copy(p, target_path)\n",
    "            print(f\"Copied to {target_path}\")\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        print(f\"WARNING: {critic_filename} not found. Ensure it is in the notebook directory or project root.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537af7b",
   "metadata": {},
   "source": [
    "## 3. Training Script\n",
    "We will run the training logic directly here to allow monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b35c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from core.modeling_bicameral import convert_to_bicameral\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import importlib.util\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# Smart Device Selection\n",
    "DEVICE = \"cpu\"\n",
    "USE_DTYPE = torch.float32\n",
    "\n",
    "# 1. Try TPU\n",
    "if importlib.util.find_spec(\"torch_xla\") is not None:\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        DEVICE = xm.xla_device()\n",
    "        USE_DTYPE = torch.bfloat16 # TPUs work best with bfloat16\n",
    "        print(f\"Selected Device: TPU ({DEVICE})\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# 2. Try GPU if no TPU\n",
    "if str(DEVICE) == \"cpu\" and torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    USE_DTYPE = torch.float16 # GPUs work well with fp16\n",
    "    print(f\"Selected Device: GPU ({torch.cuda.get_device_name(0)})\")\n",
    "\n",
    "if str(DEVICE) == \"cpu\":\n",
    "    print(\"Selected Device: CPU (Warning: Slow)\")\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading model with dtype={USE_DTYPE}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=USE_DTYPE)\n",
    "model = convert_to_bicameral(model, use_acc=True, use_soft_routing=True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(\"Model converted to Bicameral (ACC + Soft Routing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Datasets (Synthetic Prompts)\n",
    "logic_prompts = [\n",
    "    \"Solve this math problem: 24 * 7 + 12\",\n",
    "    \"Calculate the integral of x squared\",\n",
    "    \"If P implies Q and Q implies R, then\",\n",
    "    \"Write a python function to sort a list\",\n",
    "    \"Explain the theory of relativity step by step\",\n",
    "    \"Derive the quadratic formula\",\n",
    "    \"Logic puzzle: Three people enter a room...\",\n",
    "    \"Analyze the time complexity of bubble sort\",\n",
    "    \"What is the capital of France and its population?\",\n",
    "    \"Debug this following C++ code snippet\"\n",
    "] * 50  # Repeat for dataset size\n",
    "\n",
    "creative_prompts = [\n",
    "    \"Write a poem about a lonely robot\",\n",
    "    \"Describe a color that doesn't exist\",\n",
    "    \"Invent a new mythology for Mars\",\n",
    "    \"Write a story starting with 'The clock struck 13'\",\n",
    "    \"Imagine a world where sound is visible\",\n",
    "    \"Compose a song lyric about rain\",\n",
    "    \"Describe the taste of starlight\",\n",
    "    \"Write a dialogue between a cat and a ghost\",\n",
    "    \"Create a recipe for a magical potion\",\n",
    "    \"Stream of consciousness about the ocean\"\n",
    "] * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase(phase_name, data, target_id, model, tokenizer, steps=500):\n",
    "    print(f\"\\n=== Starting {phase_name} Phase (Target D={target_id}) ===\")\n",
    "\n",
    "    # Set Forced Mode for Training\n",
    "    mode_str = \"LOGIC\" if phase_name == \"Logic\" else \"CREATIVE\"\n",
    "    for layer in model.model.layers:\n",
    "        layer.mlp.forced_mode = mode_str\n",
    "\n",
    "    # Freeze/Thaw Parameters\n",
    "    trainable_params = []\n",
    "    for n, p in model.named_parameters():\n",
    "        # Train relevant expert and the ACC parameters\n",
    "        if (phase_name.lower() in n) or (\"router\" in n) or (\"inhibitory\" in n):\n",
    "            p.requires_grad = True\n",
    "            trainable_params.append(p)\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    optimizer = optim.AdamW(trainable_params, lr=5e-5)\n",
    "\n",
    "    # Tracking\n",
    "    loss_history = []\n",
    "    id_history = []\n",
    "\n",
    "    pbar = tqdm(range(steps))\n",
    "    data_iter = iter(data * 100) # Infinite loop hack\n",
    "\n",
    "    for i in pbar:\n",
    "        text = next(data_iter)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(DEVICE)\n",
    "\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        lm_loss = outputs.loss\n",
    "\n",
    "        # Topological Loss\n",
    "        # Ensure we capture IDs from forward pass\n",
    "        ids = [layer.mlp.cached_id for layer in model.model.layers if layer.mlp.cached_id is not None]\n",
    "        if ids:\n",
    "            avg_id = torch.stack(ids).mean()\n",
    "            # Stronger penalty: Weighted MSE\n",
    "            topo_loss = 2.0 * (avg_id - target_id)**2\n",
    "        else:\n",
    "            avg_id = torch.tensor(2.0)\n",
    "            topo_loss = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "        total_loss = lm_loss + topo_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_history.append(total_loss.item())\n",
    "        id_history.append(avg_id.item())\n",
    "\n",
    "        pbar.set_postfix({'Loss': f\"{total_loss.item():.3f}\", 'ID': f\"{avg_id.item():.2f}\"})\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            # Live Plotting\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(id_history, label='Avg Intrinsic Dimension')\n",
    "            plt.axhline(y=target_id, color='r', linestyle='--', label=f'Target D={target_id}')\n",
    "            plt.title(f\"{phase_name} Phase Training\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    return id_history\n",
    "\n",
    "\n",
    "# --- RUN TRAINING ---\n",
    "\n",
    "# Phase 1: Logic (Target D=1.5)\n",
    "logic_ids = train_phase(\"Logic\", logic_prompts, 1.5, model, tokenizer, steps=300)\n",
    "\n",
    "# Phase 2: Creative (Target D=2.5)\n",
    "creative_ids = train_phase(\"Creative\", creative_prompts, 2.5, model, tokenizer, steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde6cba",
   "metadata": {},
   "source": [
    "## 4. Save and Download Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"smollm3_bicameral_diff\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Zip for download\n",
    "get_ipython().system(f'zip -r {save_path}.zip {save_path}')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(f\"{save_path}.zip\")\n",
    "except ImportError:\n",
    "    print(f\"Local environment detected. Files saved to {save_path}.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}